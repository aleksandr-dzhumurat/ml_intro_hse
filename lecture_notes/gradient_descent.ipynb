{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24b890fa",
   "metadata": {},
   "source": [
    "# Градиентное обучение\n",
    "\n",
    "## Линейная регрессия: точное решение\n",
    "\n",
    "Вспомним, что задачу машинного обучения сводят к задаче минимизации эмпирического риска $Q(h(x, \\theta), D)$ где\n",
    "$$\n",
    "Q_{\\text{emp}}(h) = \\frac{1}{N}\\sum_{i=1}^{N}L(h(x_i, \\theta), y_i)\n",
    "$$\n",
    "\n",
    "В формуле выше $D$ - это накопленный опыт, то есть обучающая выборка а $h(x, \\theta)$ - семейство функций, среди которых ищем решение.\n",
    "\n",
    "лучшую модель $\\hat{h}$ из семейства $h$ мы находим как точку, в которой эмпирический риск достигает минимума.  \n",
    "$$\n",
    "\\hat{h} = \\arg\\min Q_{\\text{emp}}(h, \\theta)\n",
    "$$\n",
    "\n",
    "Точка минимума эмпирического риска является решением уравнения\n",
    "$$\n",
    "\\frac{\\partial Q_{\\text{emp}}}{\\partial h} = 0\n",
    "$$\n",
    "\n",
    "Применительно к задаче **линейной регресии** получаем следующее: хотим восстановить функцию $h(x_i)$ в виде *линейной комбинации* (т.е. суммы с некоторыми весами \"важности\") признаков объекта $(x_1,\\ldots, x_n)$. Сами признаки называются *предикторами*:\n",
    "\n",
    "$$\n",
    "\\forall x_i: h(x) = w_0 + w_1x_i^1 + \\ldots + w_nx_i^n = \\sum_{j=1}^{n}w_jx_i^j = \\overline{x}_i^T\\overline{w}\n",
    "$$\n",
    "\n",
    "Обратим внимание на специальный признак $w_0$ - он называется intercept. Позволяет более точно решать задачу, как видно по картинке\n",
    "\n",
    "![intercept_apply](img/intercept_apply.png)\n",
    "\n",
    "Мера качества $L$ для задачи регрессии - квадрат разности между фактическим значением и прогнозом. \n",
    "$$\n",
    "L(h(x_i, \\theta), y_i) = \\left(y_i - h(x_i, \\theta)\\right)^2 = \\left(y_i -  \\sum_{j=1}^{n}w_jx_i^j\\right)^2\n",
    "$$\n",
    "\n",
    "Тогда эмпирический риск вычисляется по формуле:\n",
    "$$\n",
    "Q_{\\text{emp}} = \\frac{1}{N}\\sum_{i=1}^{N}(y_i - \\hat{y_i})^2 = \\frac{1}{N}\\sum_{i=1}^{N}(y_i - \\overline{x}_i^T\\overline{w})^2 = \\frac{1}{2N}||\\overline{Y}-\\overline{X}^T\\overline{w}||^2 = \\frac{1}{2N}\\left(\\overline{Y}-\\overline{X}^T\\overline{w}\\right)^T\\left(\\overline{Y}-\\overline{X}^T\\overline{w}\\right)\n",
    "$$\n",
    "\n",
    "Такой вид функции потерь называется RSS - *resudal squares sum*, на русский переводится как *остаточная сумма квадратов*. Нам нужно найти оптимальное значение весов $w_1,\\ldots,w_n$\n",
    "\n",
    "Вспомним, что $\\hat{y_i}$ - ответ нашего алгоритма машинного обучения $h(x, \\theta)$ на примере $x_i$. Чем *больше* значение функции потерь $L$ (т.е. чем *ближе оно к нулю*, т.к. берём со знаком минус) тем лучше наша модель повторяет опыт (в виде матрицы объекты-признаки) $X \\in m \\times n$ где $m$ - количество примеров в обучающей выборке, а $n$ - размерность пространства признаков. $w$ - это вектор параметров модели, который хотим обучить.\n",
    "\n",
    "Мимнимум эмпирического риска можно найти аналитически, он достигается в точке $\\overline{w}$\n",
    "$$\n",
    "\\overline{w} = \\left(X^TX\\right)^{-1}X^T\\overline{y}\n",
    "$$\n",
    "\n",
    "В [домашней работе](../jypyter_notebooks/Part_II_gradient_descent.ipynb) мы запрограммируем такое решение с помощью numpy и сравним его с готовым решением из sklearn.\n",
    "\n",
    "## Линейная регрессия: алгоритм градиентного спуска\n",
    "\n",
    "У аналитического решения есть ряд недостатков\n",
    "* вычислительная сложность алгоритма матричного перемножения составляет $O(n^3)$, где $n$ - размерность матрицы. При увеличении размерности матрицы в 10 раз сложность вычислений увеличивается в $10^3=1000$ раз\n",
    "* **неустойчивость вычислений** - пытаемся найти обратную матрицу, которая может не существовать, в этом случае в алгоритме нахождения обратной матрицы возникает деление на ноль\n",
    "\n",
    "С неустойчивостью вычислений, например, связано предупреждение **LinAlgWarning:**. Пример такой \"неудачной\" матрицы:\n",
    "$$\n",
    "X^TX = \n",
    "\\left[\n",
    "\\begin{array}{cc}\n",
    "5 & 25 \\\\\n",
    "2 & 10 \n",
    "\\end{array}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "Допустим, хотим вычислить коэффициенты аналитически. Если попытаемся найти обратную матрицу $(X^TX)^{-1}$, мы получим сообщение об ошибке - попробуйте запустить код ниже - увидите ошибку `LinAlgError: Singular matrix`.\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# матрица из примера\n",
    "X = np.array([\n",
    "    [5, 25],\n",
    "    [2, 10]\n",
    "])\n",
    "# пытаемся найти обратную\n",
    "np.linalg.inv(X)\n",
    "```\n",
    "\n",
    "Это очень печально - значит, мы не всегда можем применять аналитическую формулу $\\overline{w} = \\left(X^TX\\right)^{-1}X^T\\overline{y}$ для нахождения коэффициентов $\\overline{w}$ Как же быть, если задачу решать все равно надо?\n",
    "\n",
    "Решение этих проблем нашли математики - давайте вычислять коэффициенты линейной регрессии не аналитически, а с помощью приближённых численных методов. Тогда не надо будет перемножать матрицы или находить обратные матрицы. Самый простой и эффективный из этих методов называется методом *градиентного спуска*. Суть метода состоит в обновлении параметров модели $w$ по маленьким шажкам (вместо того, чтобы находить их сразу) - это и есть градиентный спуск.\n",
    "\n",
    "Каждый такой шажок назвается \"итерация\".\n",
    "\n",
    "Мы знаем, что коэффициенты обучаются при помощи минимизации функции ошибок:\n",
    "\n",
    "$$\n",
    "L(y,w) = \\sum_{i=1}^{N}\\left(y_i - \\hat{y_i}\\right)^2\n",
    "$$\n",
    "\n",
    "Эта функция квадратичная - следовательно, имеет форму параболы (или \"аналог\" параболы для многомерного случая). Минимум параболы соответствует минимуму ошибки - давайте как-то понемногу \"подкручивать\" параметры, чтобы по шажкам спуститься в точку, где ошибка будет минимальной - в этой точке и находятся параметры $w$, которые мы ищем. Правила обновления весов должны быть очень простыми и не содержать матричных перемножений\n",
    "\n",
    "![grad_descent_single_measure](img/grad_descent_single_measure.png)\n",
    "\n",
    "В трёхмерном случае картинка более красивая - мы движемся как бы ландшафту и хотим найти самую нижнюю точку на этом ландшафте:\n",
    "\n",
    "![grad_descent_single_measure](img/grad_descent_multi_measure.png)\n",
    "\n",
    "На семинаре мы запустим шажки градиентного спуска (те самые итерации) - для этого используем готовый класс [SGDRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html) и функцию [partial_fit](https://scikit-learn.org/0.15/modules/scaling_strategies.html#incremental-learning)\n",
    "\n",
    "\"Маленькие шажки\", которыми мы двигаемся к оптимальному решению в виде формулы выглядят следующим образом:\n",
    "$$\n",
    "w^{k+1} = w^k - \\eta\\nabla L(w)\n",
    "$$\n",
    "Переменная $\\eta$ в формуле - т.н. *шаг градиентного спуска*. $\\nabla L(w)$ - вектор градиента функции. Этот вектор обладает следующими свойствами: \n",
    "\n",
    "* имеет размерность вектора параметров. Если у модели два параметра $[w_0, w_1]$ - в векторе градиента будет два элемента\n",
    "* элемент градиента под номером $i$ - это частная производная (вспоминаем математику за 11 класс и [смотрим в Википедию](https://ru.wikipedia.org/wiki/Производная_функции ) ) функции потерь $L(y, w)$ по параметру $w_i$\n",
    "\n",
    "Вектор антиградиента всегда направлен в сторону уменьшения функции - в этом и есть всё волшебство! Мы будем двигаться в сторону минимума функции ошибки, потому что знаем как туда попасть - надо следовать по антиградиенту.\n",
    "\n",
    "На картинке одномерный случай. Синяя стрелка - градиент, красная - антиградиент. Видно, что если двигаться по вектору антиградиента, то свалимся в минимум функции за конечное число шагов\n",
    "![grad_vector](img/grad_vector.png)\n",
    "\n",
    "В трёхмерном случае (эллипс - проекция трёхмерной фигуры функции потерь на плоскость) алгоритм визуально выглядит как на картинке ниже - оптимальное значение функции находится в центре концентрицеских эллипсов:\n",
    "1. Вычисляем вектор анти-градиента\n",
    "1. Делаем шаг по вектору, пересчитываем вектор в новой точке\n",
    "1. Шагаем, пока не попадём в финальную точку, откуда шаг сделать не получится - это и будет минимум целевой функции.\n",
    "\n",
    "![grad_descent](img/grad_descent_intuit.png)\n",
    "\n",
    "\n",
    "Как туда добраться, двигаясь маленькими шажками?\n",
    "\n",
    "1. Стартуем алгоритм в случайной начальной точке $x^0$  \n",
    "1. Вычисляем направление антиградиента $-f'(x^0)$ (буквально: производная со знаком \"минус\")\n",
    "1. Перемещаемся по направлению градиента в точку $x^1 = x^0 - \\eta f'(x^0)$\n",
    "1. Повторяем шаги (1-3) для попадания в точку $x^2$\n",
    "1. $\\ldots$\n",
    "1. Profit! Достигли оптимальной точки $x^*$\n",
    "\n",
    "Алгоритм выше - универсальный, он позволяет найти точку минимума любой функции $f(x)$. А как же нам найти минимум функции качества линейной регрессии $L$\n",
    "\n",
    "$$\n",
    "L(y,w) = \\sum_{i=1}^{N}\\left(y_i - \\hat{y_i}\\right)^2\n",
    "$$\n",
    "\n",
    "Чтобы реализовать алгоритм градиентного спуска, выпишем частные прозводные функции качества линейной регрессии $L$ для параметров $\\overline{w} = [w_1,\\ldots,w_m]$ в простейшем случае $n=1$, то есть для одного обучающего примера (одного наблюдения):\n",
    "$$\n",
    "\\left\\{\n",
    "\\begin{array}{cc}\n",
    "\\frac{\\partial L}{\\partial w_0} = 2\\cdot(-1)\\cdot1\\cdot (y - (w_0x_0 + \\ldots+w_mx_m)) &\\\\\n",
    "\\frac{\\partial L}{\\partial w_k} = 2\\cdot(-1)\\cdot x_k \\cdot (y - (w_0x_0 + \\ldots+w_mx_m)) &  k\\neq 0\\\\\n",
    "\\end{array}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "В формуле все обозначения вам известны\n",
    "\n",
    "* $w_0, \\ldots, w_m$ - коэффициенты линейной регрессиии $m$ - количество фичей\n",
    "* $x_0, \\ldots, x_m$ - фичи.\n",
    "\n",
    "Эту формулу с частными производными можно легко обобщить на случай, когда в обучающей выборке не один объект, а $n$ - просто добавляем сумму по всем объектам от 1 до $n$:\n",
    "$$\n",
    "\\left\\{\n",
    "\\begin{array}{cc}\n",
    "\\frac{\\partial L}{\\partial w_0} = \\frac{2}{n}\\cdot(-1)\\cdot \\sum_{i=1}^{n} 1\\cdot \\left(y_i - \\sum_{j=1}^{m}w_jx_j^i\\right) &\\\\\n",
    "\\frac{\\partial L}{\\partial w_k} = \\frac{2}{n}\\cdot(-1)\\cdot \\sum_{i=1}^{n} x_k^i \\cdot\\left(y_i - \\sum_{j=1}^{m}w_jx_j^i\\right) & k\\neq 0 \\\\\n",
    "\\end{array}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "Если $n$ большой то вычислять и обновлять градиенты можно не по всей выборке, а по  \n",
    "\n",
    "Для примера траектория градиентного спуска\n",
    "![gd_traektory](img/gd_traektory.png)\n",
    "\n",
    "Для примера траектория стохастическогом градиентного спуска\n",
    "![sgd_traektory](img/sgd_traektory.png)\n",
    "\n",
    "Что значат переменные в этой формуле?\n",
    "* $n$ - число точек в обучающей выборке\n",
    "* $m$ - число фичей в датасете\n",
    "* $k$ - номер коэффициента в списке параметров линейной регрессии $w = [w_0,\\ldots,w_m]$\n",
    "* $y_i$ - значение целевой переменной на объекте обучающей выборки под номером $i$\n",
    "* $x_j^i$ - значение фичи под номером $j$ на объекте обучающей выборки под номером $i$\n",
    "\n",
    "Алгоритм градиентного спуска следующий:\n",
    "\n",
    "1. Вычислить градиент $\\nabla L(w)$\n",
    "1. Вычислить новый вектор  $w^{k+1} = w^k - \\eta\\nabla L(w)$\n",
    "1. Повторять пункты до тех пор, пока $w^{k+1}$ и $w^{k}$ не сойдутся вместе, то есть вектор весов перестанет обновляться\n",
    "\n",
    "Чтобы лучше понять этот метод, на семинаре реализуем его \"с нуля\" на языке Python\n",
    "\n",
    "**Особенности градиентного спуска**\n",
    "\n",
    "1. Нужно подбирать параметр $\\eta$. Еcли выбрать параметр слишком малым, то обучение регрессии будет происходить слишком медленно. Если слишком большим - вычисления не сойдутся к оптимуму. Вариант решения - адаптивный выбор величины шага\n",
    "1. Долгие вычисления, если размер выборки $n$ становится большим. В этом случае мы можем вычислять градиент не по всей выборке за один шаг, а по одному случайному элементу выборки - в этом случае вычислений значительно меньше.\n",
    "\n",
    "Кроме того, градиент можно считать не только по одному объекту, но и по случайной подвыборке (батчу). Такая модификация алгоритма называется градиентным спуском по мини-батчам.\n",
    "\n",
    "\n",
    "Хорошая теория [тут](http://www.machinelearning.ru/wiki/images/6/68/voron-ML-Lin.pdf). Неплохая статья с разбором [формул обновления весов](https://medium.com/@lachlanmiller_52885/machine-learning-week-1-cost-function-gradient-descent-and-univariate-linear-regression-8f5fe69815fd). Видео от [специализации ШАД на coursera](https://ru.coursera.org/lecture/supervised-learning/gradiientnyi-spusk-dlia-linieinoi-rieghriessii-adARX)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0ab654",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
