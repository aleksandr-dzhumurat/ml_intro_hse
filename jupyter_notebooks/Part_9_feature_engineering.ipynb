{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Part 9: feature engineering.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Feature egineering\n",
        "\n",
        "Подключаем google диск"
      ],
      "metadata": {
        "id": "ngEBDlPa5Q2Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import os\n",
        "import shutil \n",
        "\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)  # гарантируем воспроизводимость\n",
        "\n",
        "logging.basicConfig(format='%(asctime)s : %(message)s', level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.info('Инициализировали логгер')\n",
        "\n",
        "ROOT_DIR = '/content/drive' \n",
        "drive.mount(ROOT_DIR)\n",
        "logger.info('Подключили диск')\n",
        "\n",
        "root_data_dir = os.path.join(ROOT_DIR, 'MyDrive', 'hse_nlp_2021')\n",
        "if not os.path.exists(root_data_dir):\n",
        "  raise RuntimeError('Отсутствует директория с данными')\n",
        "else:\n",
        "  logger.info('Содержимое директории %s: %s', root_data_dir, os.listdir(root_data_dir))"
      ],
      "metadata": {
        "id": "NFkdRiFT5RLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Загружаем датасет с результатами модерации контента"
      ],
      "metadata": {
        "id": "gCFedRO56u8q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "input_dataset_df = pd.read_csv(os.path.join(root_data_dir, 'final_dataset.zip'), compression='zip')\n",
        "logger.info('num rows %d', input_dataset_df.shape[0])\n",
        "\n",
        "input_dataset_df.head()"
      ],
      "metadata": {
        "id": "XR46GF8Q64_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Датасет с текстами"
      ],
      "metadata": {
        "id": "_3poBonkKrIu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ocr_dataset_df = (\n",
        "    pd\n",
        "    .read_csv(os.path.join(root_data_dir, 'ocr_dataset.zip'), compression='zip')\n",
        "    .head(10000)\n",
        ")\n",
        "\n",
        "ocr_dataset_df.head()"
      ],
      "metadata": {
        "id": "cgn25YbRKrYe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Визуализации данных\n",
        "\n",
        "Чаще всего используется визуализация `scatter plot`"
      ],
      "metadata": {
        "id": "E3__EeFM6TI4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "plt.scatter(input_dataset_df['num_comments'], input_dataset_df['num_shares'])"
      ],
      "metadata": {
        "id": "oSM6UoZ26XWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Строим гистограмму"
      ],
      "metadata": {
        "id": "Eh4p0e-F6_45"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(input_dataset_df['num_comments'], bins=10, density=True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "L9yBIb-b6-sQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Выглядит странно, как будто есть небольшое количество данных, больших по модулю и большое количество данных около-нулевых - такие большие элементы называют выборасами (outliers)\n",
        "\n",
        "Выявляем выбросы"
      ],
      "metadata": {
        "id": "woehAAJP7isN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_dataset_df['num_comments'].describe(percentiles=[.5, .95, .99])"
      ],
      "metadata": {
        "id": "JSz2zY887m21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Как ещё проверить на выбросы? С помощью `.boxplot()`"
      ],
      "metadata": {
        "id": "XlUvafGqFyik"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_dataset_df[['num_comments']].boxplot()"
      ],
      "metadata": {
        "id": "jaNnyvPIFzI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Переделываем гистограмму (без выбросов)"
      ],
      "metadata": {
        "id": "_kSC8ZF96-8K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(\n",
        "    input_dataset_df[input_dataset_df['num_comments'] < 12]['num_comments'],\n",
        "    bins=10, density=True\n",
        ")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Kzs4BrUF6XZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Трансформации данных\n",
        "\n",
        "Получили т.н. распределение с \"тяжёлым хвостом\"\n",
        "\n",
        "Данные можно центрировать и снизить дисперсию - такое преобразование называется z-score"
      ],
      "metadata": {
        "id": "4k0dUSE08dMm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "transformed_z_score = (\n",
        "    StandardScaler()\n",
        "    .fit_transform(\n",
        "        input_dataset_df[input_dataset_df['num_comments'] < 12]['num_comments']\n",
        "        .values.reshape(-1, 1)\n",
        "    )\n",
        ")\n",
        "\n",
        "plt.hist(transformed_z_score, bins=10, density=True)\n",
        "plt.plot()"
      ],
      "metadata": {
        "id": "weuBfPJo6Xb5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Можно ли применять логарифмирование всегда?\n",
        "\n",
        "Давайте попробуем"
      ],
      "metadata": {
        "id": "s6E68-bf-DNN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(np.log(transformed_z_score), bins=5, density=True)\n",
        "plt.plot()"
      ],
      "metadata": {
        "id": "unI3zzJv6Xek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Получили какой-то пропуск в данных, почему так вышло?"
      ],
      "metadata": {
        "id": "O5owlyVQYnXm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logger.info('range=[%.4f, %.4f]', transformed_z_score.min(), transformed_z_score.max())\n",
        "\n",
        "carrier = np.linspace(transformed_z_score.min(), transformed_z_score.max())\n",
        "\n",
        "plt.plot(\n",
        "    carrier, np.log(carrier)\n",
        ")"
      ],
      "metadata": {
        "id": "PKTUVCec6Xhi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Получается, в numpy логарифм для отрицательных чисел не определён - тут нужно применить другое масштабирование, например min-max.\n",
        "\n",
        "Получается, что логарифмирование (и извлечение квадратного корня нельзя применять после вычисления z-score)\n",
        "\n",
        "Нужно выбрать один из вариантов \n",
        "* сначала логарифмировать, а потом применить z-score\n",
        "* только z-score\n",
        "*только логарифмирование"
      ],
      "metadata": {
        "id": "2NUqGpjPEWy7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "transformed_z_score = (\n",
        "    MinMaxScaler()\n",
        "    .fit_transform(\n",
        "        input_dataset_df[input_dataset_df['num_comments'] < 12]['num_comments']\n",
        "        .values.reshape(-1, 1)\n",
        "    )\n",
        ")\n",
        "\n",
        "plt.hist(transformed_z_score, bins=10, density=True)\n",
        "plt.plot()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "k4U55F7cEyef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Пропущенные значения\n",
        "\n",
        "Проверка на пропущенные значения"
      ],
      "metadata": {
        "id": "B1d-xlayExRn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "null_values_share = (\n",
        "    input_dataset_df['declined_reason']\n",
        "    .isna()\n",
        "    .value_counts(normalize=True)\n",
        "    .to_dict()\n",
        "    .get(True, 0.0)\n",
        ")\n",
        "logger.info('Доля пропущенных значений %.4f', null_values_share)"
      ],
      "metadata": {
        "id": "v80m2JgWFjh9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Заменяем `NULL` на пустую строку"
      ],
      "metadata": {
        "id": "dOBdNPk3HThE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_dataset_df['declined_reason'].fillna(value='Value missed', inplace=True)\n",
        "\n",
        "null_values_share = (\n",
        "    input_dataset_df['declined_reason']\n",
        "    .isna()\n",
        "    .value_counts(normalize=True)\n",
        "    .to_dict()\n",
        "    .get(True, 0.0)\n",
        ")\n",
        "logger.info('Доля пропущенных значений %.4f', null_values_share)"
      ],
      "metadata": {
        "id": "QD259jiuHaXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Кодирование категориальных фичей\n",
        "\n",
        "Кодируем категориальные фичи"
      ],
      "metadata": {
        "id": "9N2yXORAHvs8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "le = LabelEncoder()\n",
        "\n",
        "le_transformed = le.fit_transform(input_dataset_df['declined_reason'])\n",
        "\n",
        "pd.Series(le_transformed).value_counts().head()"
      ],
      "metadata": {
        "id": "8PwiSyz3H4Pu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Другой (более подходящий) вид энкодинга"
      ],
      "metadata": {
        "id": "33AO7wjdInGJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "ohe = OneHotEncoder()\n",
        "\n",
        "ohe_transformed = ohe.fit_transform(input_dataset_df[['declined_reason']])\n",
        "\n",
        "ohe_transformed"
      ],
      "metadata": {
        "id": "grtMs14GI3_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Текстовые фичи\n",
        "\n",
        "## Bag of words"
      ],
      "metadata": {
        "id": "lowFEQvzJ-ER"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(\n",
        "    analyzer='word',\n",
        "    lowercase=True,\n",
        "    token_pattern=r'\\b[\\w\\d]{3,}\\b',\n",
        "    min_df=0.001\n",
        ")\n",
        "\n",
        "bow_matrix = vectorizer.fit_transform(ocr_dataset_df.text.values)\n",
        "bow_matrix"
      ],
      "metadata": {
        "id": "XR1gNndRKF-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "По sparse матрице можно найти попарное расстояние между текстами"
      ],
      "metadata": {
        "id": "WjvMeuFKMOvs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import pairwise_distances\n",
        "\n",
        "text_distance = 1 - pairwise_distances(bow_matrix, metric=\"cosine\")\n",
        "\n",
        "text_distance.shape"
      ],
      "metadata": {
        "id": "38a99UAMMfdp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Мы получили квадратную матрицy, которая содержит столько строк и столбцов, сколько документов в нашем корпусе (наборе текстов).\n",
        "\n",
        "Зачем нужна такая матрица? Например, можно искать \"похожие\" тексты для задачи рекомендаций"
      ],
      "metadata": {
        "id": "ldr3ZZm0Mc_x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "source_tweet_index = 14  # тут может быть любое число в диапазоне от 1 до 1000\n",
        "\n",
        "print(ocr_dataset_df.iloc[source_tweet_index].text)"
      ],
      "metadata": {
        "id": "0lZqIVAS5o75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Находим ближайший похожий текст\n",
        "\n",
        "отсортируем твиты по “похожести” - чем похожее на `source_tweet_index`, тем ближе к началу списка sorted_similarity"
      ],
      "metadata": {
        "id": "bG_7aER36SBB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_similarity = np.argsort(-1 * text_distance[source_tweet_index,:])\n",
        "\n",
        "sorted_similarity"
      ],
      "metadata": {
        "id": "5yDFHfG46XeR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Теперь распечаем \"схожие\" тексты"
      ],
      "metadata": {
        "id": "Mw9GHt6s6-SQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for content_index in sorted_similarity[:4]:\n",
        "  print(ocr_dataset_df.iloc[content_index].text)\n",
        "  print('-------------\\n-------------')"
      ],
      "metadata": {
        "id": "7Pd1j_I27D3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Препроцессинг текста\n",
        "\n",
        "Устанавливаем NLTK. Для начала готовим директорию для данных"
      ],
      "metadata": {
        "id": "gMpcMke_8ErE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk_data_dir = os.path.join(ROOT_DIR, 'MyDrive', 'hse_nlp_2021', 'nltk_data')\n",
        "if not os.path.exists(nltk_data_dir):\n",
        "  os.makedirs(nltk_data_dir)\n",
        "  logger.info('Директория %s создана', nltk_data_dir)\n",
        "logs_dir = os.path.join(ROOT_DIR, 'MyDrive', 'hse_nlp_2021', 'logs')\n",
        "if not os.path.exists(logs_dir):\n",
        "  os.makedirs(logs_dir)\n",
        "logger.info('Подготовили директорию для nltk %s', nltk_data_dir)"
      ],
      "metadata": {
        "id": "EGOL2Tzv-IcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Обязательно фиксируем версию! Иначе будут косяки"
      ],
      "metadata": {
        "id": "ovlUmPem-Yld"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk==3.6.2"
      ],
      "metadata": {
        "id": "BbqOSM7H-enl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('punkt', download_dir=nltk_data_dir)\n",
        "nltk.data.path.append(nltk_data_dir) # тут почему-то корневую надо указывать ¯\\_(ツ)_/¯"
      ],
      "metadata": {
        "id": "ELNptAEW-tGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "sample_str = ocr_dataset_df.text.values[0]\n",
        "\n",
        "print('== Исходный текст== \\n%s\\n\\n' % sample_str)\n",
        "\n",
        "tokenized_str = nltk.word_tokenize(sample_str)\n",
        "print('== Токенизированный текст==\\n%s' % tokenized_str)"
      ],
      "metadata": {
        "id": "hG4GNJT38Unx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Видим знаки препинания, надо их отфильтровать"
      ],
      "metadata": {
        "id": "a6Egvl76_2C6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string  # стандартный модуль\n",
        "\n",
        "tokens = [i.lower() for i in tokenized_str if ( i not in string.punctuation )]\n",
        "print(tokens)"
      ],
      "metadata": {
        "id": "NgYYdKmP_8ty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Стоп-слова"
      ],
      "metadata": {
        "id": "6JNqTumjAKAg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = [\n",
        "    'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\",\n",
        "    'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers',\n",
        "    'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which',\n",
        "    'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been',\n",
        "    'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if',\n",
        "    'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between',\n",
        "    'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out',\n",
        "    'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why',\n",
        "    'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not',\n",
        "    'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'shold',\n",
        "    \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\",\n",
        "    'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\",\n",
        "    'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\",\n",
        "    'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"\n",
        "]\n",
        "\n",
        "filtered_tokens = [i for i in tokens if ( i not in stop_words )]\n",
        "\n",
        "print(filtered_tokens)"
      ],
      "metadata": {
        "id": "iREyrgcZALhF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Соединяем в одну функцию"
      ],
      "metadata": {
        "id": "3SkOknHoAVaF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_text(raw_text: str):\n",
        "    \"\"\"Функция для токенизации текста\n",
        "    \n",
        "    :param raw_text: исходная текстовая строка\n",
        "    \"\"\"\n",
        "    filtered_tokens = []\n",
        "    \n",
        "    filtered_tokens = [i.lower() for i in raw_text.split() if ( i not in string.punctuation )]\n",
        "    filtered_tokens = [i for i in filtered_tokens if ( i not in stop_words )]\n",
        "    filtered_tokens = [i for i in filtered_tokens if ( len(i) > 2 )]\n",
        "    \n",
        "    return filtered_tokens\n",
        "\n",
        "# применяем функцию в датафрейму с помощью метода .apply()\n",
        "tokenized_tweets= ocr_dataset_df.text.apply(tokenize_text)\n",
        "\n",
        "# добавляем новую колонку в исходный датафрейм\n",
        "ocr_dataset_df = ocr_dataset_df.assign(\n",
        "    tokenized=tokenized_tweets\n",
        ")\n",
        "\n",
        "ocr_dataset_df.tokenized.head()"
      ],
      "metadata": {
        "id": "mN8_tDcSAZ_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Эмбеддинги Word2Vec"
      ],
      "metadata": {
        "id": "tnJK9SOU7y24"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "import logging\n",
        "\n",
        "texts = ocr_dataset_df.tokenized.values\n",
        "\n",
        "model = Word2Vec(texts, size=10, window=7, min_count=2, workers=4, iter=10, sg=0)"
      ],
      "metadata": {
        "id": "9jPa2NDu73Q7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Эмбеддинги обучились, посмотрим как они выглядят"
      ],
      "metadata": {
        "id": "j6Tlis5AJ7Kp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv.get_vector('android')"
      ],
      "metadata": {
        "id": "PbgavG2ZKFi7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Проверяем эмбеддинги - ищем эмбеддинг, самый похожий на эмбеддинг слова Biden"
      ],
      "metadata": {
        "id": "NHXMsjNGBxHT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv.most_similar('biden')"
      ],
      "metadata": {
        "id": "1YgKqeLkB934"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Включаем GPU\n",
        "\n",
        "Подробнее о модели [тут](https://pytorch.org/hub/pytorch_vision_resnet/)\n",
        "\n",
        "![gpu_on_colab](img/gpu_on_colab_gui.png)"
      ],
      "metadata": {
        "id": "N1JAQPnEtrUr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.models import resnet18\n",
        "\n",
        "TORCH_MODELS_DIR = os.path.join(ROOT_DIR, 'MyDrive', 'hse_nlp_2021', 'torch_models')\n",
        "try:\n",
        "  os.mkdir(TORCH_MODELS_DIR)\n",
        "except FileExistsError as e:\n",
        "  logger.info(e)\n",
        "\n",
        "os.environ['TORCH_HOME'] = TORCH_MODELS_DIR # TORCH_MODEL_ZOO is deprecated\n",
        "rn18 = resnet18(pretrained=True)\n",
        "\n",
        "# запускаем вычисления на GPU\n",
        "# rn18 = rn18.to('cuda:0')\n",
        "logger.info('Модель загружена')"
      ],
      "metadata": {
        "id": "I90TPsOdOzoW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Эмбеддинги картинок\n",
        "\n",
        "Для начала посмотрим, какие слои есть в сети\n",
        "\n",
        "Кроме `.modules` можно было воспользоваться `.named_children()`"
      ],
      "metadata": {
        "id": "3ajlIo1veOAh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for layer in rn18._modules:\n",
        "  print(layer)"
      ],
      "metadata": {
        "id": "JsS8ABYWjZPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Нам нужен слой `layer4`\n",
        "\n",
        "Каждый слой это по сути массив c весами модели - нам нужно оставить все слои ДО того слоя, который нас интересует"
      ],
      "metadata": {
        "id": "OR0Y-xXhjcvj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "\n",
        "class FeatureExtractor(nn.Module):\n",
        "    def __init__(self, output_layer, torch_model):\n",
        "        super().__init__()\n",
        "        self.output_layer = output_layer\n",
        "        self.pretrained = torch_model\n",
        "        self.children_list = []\n",
        "        for n,c in self.pretrained.named_children():\n",
        "            self.children_list.append(c)\n",
        "            if n == self.output_layer:\n",
        "                logger.info('final layer archived: %s', output_layer)\n",
        "                break\n",
        "\n",
        "        self.net = nn.Sequential(*self.children_list)\n",
        "        self.pretrained = None\n",
        "        \n",
        "    def forward(self,x):\n",
        "        x = self.net(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "O0vT7v73V9cd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Создаём объект-экстрактор с выборанным слоем"
      ],
      "metadata": {
        "id": "vnR8wWKSpRhV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resnet_extractor = FeatureExtractor(output_layer='avgpool', torch_model=rn18)\n"
      ],
      "metadata": {
        "id": "NAiIQvreOvL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Проверяем директорию с картинками"
      ],
      "metadata": {
        "id": "IO1wL1d4Fs2E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ROOT_MEMES_DIR = os.path.join(ROOT_DIR, 'MyDrive', 'hse_nlp_2021', 'memes')\n",
        "\n",
        "os.listdir(ROOT_MEMES_DIR)[:10]"
      ],
      "metadata": {
        "id": "sJXkiao5FtGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "\n",
        "filename = os.path.join(ROOT_MEMES_DIR, '7f3ywc.jpeg')\n",
        "\n",
        "input_image = Image.open(filename)"
      ],
      "metadata": {
        "id": "cTVdn7QPNG18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(input_image)"
      ],
      "metadata": {
        "id": "UDjz-L5gyur_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import no_grad, cuda\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# transform = transforms.ToTensor()\n",
        "\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "image_tensor = preprocess(input_image)\n",
        "input_batch = image_tensor.unsqueeze(0) # create a mini-batch as expected by the model\n",
        "\n",
        "# move the input and model to GPU for speed if available\n",
        "if cuda.is_available():\n",
        "    input_batch = input_batch.to('cuda')\n",
        "    resnet_extractor.to('cuda')\n",
        "\n",
        "with no_grad():\n",
        "    output = resnet_extractor(input_batch)\n",
        "    numpy_vector = output.reshape(-1).cpu().numpy()  # flatten(output)\n",
        "    print(type(output), output.size())"
      ],
      "metadata": {
        "id": "jwJLliIoV9fm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "from PIL.JpegImagePlugin import JpegImageFile\n",
        "from torch import no_grad, cuda\n",
        "from PIL import Image, UnidentifiedImageError\n",
        "\n",
        "def img2embedding(input_meme_filename: str) -> np.array:\n",
        "  OUTPUT_SHAPE = 512\n",
        "  numpy_vector = np.zeros(OUTPUT_SHAPE)\n",
        "  preprocess = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "  ])\n",
        "\n",
        "  try:\n",
        "    input_img = Image.open(input_meme_filename) # type: JpegImageFile\n",
        "  except UnidentifiedImageError:\n",
        "    return numpy_vector\n",
        "  try:\n",
        "    image_tensor = preprocess(input_img)\n",
        "  except RuntimeError:\n",
        "    #  logger.info('error with %s meme', meme_filename.split('/')[-1])\n",
        "    return numpy_vector\n",
        "  input_batch = image_tensor.unsqueeze(0) # create a mini-batch as expected by the model\n",
        "\n",
        "  # move the input and model to GPU for speed if available\n",
        "  if cuda.is_available():\n",
        "      input_batch = input_batch.to('cuda')\n",
        "      resnet_extractor.to('cuda')\n",
        "\n",
        "  with no_grad():\n",
        "      output = resnet_extractor(input_batch)\n",
        "      numpy_vector = output.reshape(-1).cpu().numpy()  # flatten(output)\n",
        "  return numpy_vector\n",
        "\n",
        "\n",
        "if os.path.exists(os.path.join(ROOT_MEMES_DIR, 'embed.npy')):\n",
        "  embeds_matrix = np.load(os.path.join(ROOT_MEMES_DIR, 'embed.npy'))\n",
        "  with open(os.path.join(ROOT_MEMES_DIR, 'file_index.pkl'), 'rb') as f:\n",
        "    file_index = pickle.load(f)\n",
        "    logger.info('files loaded from dump')\n",
        "else:\n",
        "  res = []  # тут основная информация о контенте\n",
        "  file_index = {}\n",
        "  TOP = 3092\n",
        "  error_files = []\n",
        "  logger.info('Processing started')\n",
        "  dense_index = 0\n",
        "  for f_name in os.listdir(ROOT_MEMES_DIR)[:TOP]:\n",
        "    meme_filename = os.path.join(ROOT_MEMES_DIR, f_name)\n",
        "    img_embed = img2embedding(meme_filename)\n",
        "    if img_embed.sum() == 0:\n",
        "      error_files.append(meme_filename)\n",
        "    # сохраняяем эмбеддинг (их потом схлопнем в матрицу) и отдельно индекс файла в матрице\n",
        "    res.append(img_embed)\n",
        "    file_index[dense_index] = {'f_name': f_name}\n",
        "    dense_index += 1\n",
        "    \n",
        "  if len(error_files) > 0:\n",
        "    logger.info('num errors %d', len(error_files))\n",
        "    for i in error_files:\n",
        "      os.remove(i)\n",
        "\n",
        "  embeds_matrix = np.vstack(res)\n",
        "\n",
        "logger.info(embeds_matrix.shape)"
      ],
      "metadata": {
        "id": "Q7srLON2V9jL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "np.save(os.path.join(ROOT_MEMES_DIR, 'embed.npy') , embeds_matrix)\n",
        "with open(os.path.join(ROOT_MEMES_DIR, 'file_index.pkl'), 'wb') as f:\n",
        "  pickle.dump(file_index, f)"
      ],
      "metadata": {
        "id": "MOg4T8CSy6mE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Устанавливаем umap \n",
        "\n",
        "пример из [официальной документации](https://umap-learn.readthedocs.io/en/latest/basic_usage.html)"
      ],
      "metadata": {
        "id": "m1oNoJ7r4NrQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install umap-learn==0.5.2"
      ],
      "metadata": {
        "id": "vIB3Gio54bEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import umap\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "reducer = umap.UMAP()\n",
        "\n",
        "scaled_memes_data = StandardScaler().fit_transform(embeds_matrix)\n",
        "low_rank_matrix = reducer.fit_transform(scaled_memes_data)\n",
        "logger.info('low rank matrix shape %s', low_rank_matrix.shape)"
      ],
      "metadata": {
        "id": "xkKLwLXjV9pN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "plt.scatter(\n",
        "    low_rank_matrix[:, 0],\n",
        "    low_rank_matrix[:, 1],\n",
        ")\n",
        "plt.gca().set_aspect('equal', 'datalim')\n",
        "plt.title('UMAP projection of the memes dataset', fontsize=24)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IohdafZC4BGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Получился один \"кластер\"\n",
        "\n",
        "Выполняем кластеризацию в низкоразмерном пространстве с помощью DBScan чтобы выделить метки кластеров"
      ],
      "metadata": {
        "id": "RH_jWch79kwL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "clstr = DBSCAN(eps=0.10, min_samples=4)\n",
        "classes = clstr.fit_predict(low_rank_matrix)\n",
        "logger.info('num classes %s', np.unique(classes).size)\n",
        "\n",
        "plt.scatter(\n",
        "    low_rank_matrix[:, 0],\n",
        "    low_rank_matrix[:, 1],\n",
        "    c=classes,\n",
        "    cmap='rainbow',\n",
        "    alpha=0.7,\n",
        ")\n",
        "plt.gca().set_aspect('equal', 'datalim')\n",
        "plt.title('UMAP projection of the memes dataset', fontsize=24)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Jnd8NpVU82a5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "clusters_filename = os.path.join(ROOT_MEMES_DIR, 'dbscan_clusters.csv')\n",
        "if os.path.exists(clusters_filename):\n",
        "  memes_df = pd.read_csv(clusters_filename)\n",
        "  logger.info('loaded_from %s', clusters_filename)\n",
        "else:\n",
        "  df_rows = []\n",
        "  for meme_index in range(low_rank_matrix.shape[0]):\n",
        "    df_rows.append((file_index[meme_index]['f_name'], classes[meme_index]))\n",
        "  memes_df = pd.DataFrame(df_rows, columns=['f_name', 'dbscan_cluster'])\n",
        "logger.info('%s', memes_df['dbscan_cluster'].value_counts().head(10).to_dict())\n",
        "memes_df.head(5)"
      ],
      "metadata": {
        "id": "hDMlsmsH95Wp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memes_df.to_csv(os.path.join(ROOT_MEMES_DIR, 'dbscan_clusters.csv'), index=False)"
      ],
      "metadata": {
        "id": "KQCktlEc_kDg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Видно, что есть кластер с индексом `0` где большая часть контента и меньшие по можности кластера. ДЛя сравнения визуализируем кластер c индексом `3` и кластер с индексом `4`"
      ],
      "metadata": {
        "id": "PgXRufB2wqcy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import shutil\n",
        "\n",
        "# for f_name in os.listdir(ROOT_MEMES_DIR):\n",
        "#   if f_name not in ('file_index.pkl', 'embed.npy'):\n",
        "#     os.rename(os.path.join(ROOT_MEMES_DIR, f_name), os.path.join(ROOT_MEMES_DIR, f_name.split('.')[0]+'.jpeg'))"
      ],
      "metadata": {
        "id": "FF2rdTNY-00U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image as NotebookImage\n",
        "from IPython.display import display\n",
        "\n",
        "def visualise_cluster(cluster_id: int, top=10):\n",
        "  for _, row in memes_df.query(f'dbscan_cluster == {cluster_id}').head(top).iterrows():\n",
        "    tmp_file_path = os.path.join(ROOT_MEMES_DIR, row['f_name']+'.jpeg')\n",
        "    pil_img = NotebookImage(filename=tmp_file_path, width=200)\n",
        "    display(pil_img)\n",
        "\n",
        "visualise_cluster(cluster_id=14)"
      ],
      "metadata": {
        "id": "lBYpBbhsnLyt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualise_cluster(cluster_id=16)"
      ],
      "metadata": {
        "id": "vo-c3xLGxuUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualise_cluster(cluster_id=5)"
      ],
      "metadata": {
        "id": "ihMWhvMCCaas"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "4Y_DGvqiEvK_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}