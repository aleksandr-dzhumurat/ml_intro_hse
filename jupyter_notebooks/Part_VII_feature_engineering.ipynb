{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Part VII: feature engineering.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CLyleNA_VuMM"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import os\n",
        "import shutil \n",
        "\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)  # гарантируем воспроизводимость\n",
        "\n",
        "logging.basicConfig(format='%(asctime)s : %(message)s', level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.info('Инициализировали логгер')\n",
        "\n",
        "ROOT_DIR = '/content/drive' \n",
        "drive.mount(ROOT_DIR)\n",
        "logger.info('Подключили диск')\n",
        "\n",
        "root_data_dir = os.path.join(ROOT_DIR, 'MyDrive', 'hse_nlp_2021')\n",
        "if not os.path.exists(root_data_dir):\n",
        "  raise RuntimeError('Отсутствует директория с данными')\n",
        "else:\n",
        "  logger.info('Содержимое директории %s: %s', root_data_dir, os.listdir(root_data_dir))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Включаем GPU\n",
        "\n",
        "Подробнее о модели [тут](https://pytorch.org/hub/pytorch_vision_resnet/)\n",
        "\n",
        "![gpu_on_colab](img/gpu_on_colab_gui.png)"
      ],
      "metadata": {
        "id": "N1JAQPnEtrUr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.models import resnet18\n",
        "\n",
        "TORCH_MODELS_DIR = os.path.join(ROOT_DIR, 'MyDrive', 'hse_nlp_2021', 'torch_models')\n",
        "try:\n",
        "  os.mkdir(TORCH_MODELS_DIR)\n",
        "except FileExistsError as e:\n",
        "  logger.info(e)\n",
        "\n",
        "os.environ['TORCH_HOME'] = TORCH_MODELS_DIR # TORCH_MODEL_ZOO is deprecated\n",
        "rn18 = resnet18(pretrained=True)\n",
        "\n",
        "# запускаем вычисления на GPU\n",
        "rn18 = rn18.to('cuda:0')\n",
        "logger.info('Модель загружена')"
      ],
      "metadata": {
        "id": "I90TPsOdOzoW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Эмбеддинги картинок\n",
        "\n",
        "Для начала посмотрим, какие слои есть в сети\n",
        "\n",
        "Кроме `.modules` можно было воспользоваться `.named_children()`"
      ],
      "metadata": {
        "id": "3ajlIo1veOAh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for layer in rn18._modules:\n",
        "  print(layer)"
      ],
      "metadata": {
        "id": "JsS8ABYWjZPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Нам нужен слой `layer4`\n",
        "\n",
        "Каждый слой это по сути массив c весами модели - нам нужно оставить все слои ДО того слоя, который нас интересует"
      ],
      "metadata": {
        "id": "OR0Y-xXhjcvj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "\n",
        "class FeatureExtractor(nn.Module):\n",
        "    def __init__(self, output_layer, torch_model):\n",
        "        super().__init__()\n",
        "        self.output_layer = output_layer\n",
        "        self.pretrained = torch_model\n",
        "        self.children_list = []\n",
        "        for n,c in self.pretrained.named_children():\n",
        "            self.children_list.append(c)\n",
        "            if n == self.output_layer:\n",
        "                logger.info('final layer archived: %s', output_layer)\n",
        "                break\n",
        "\n",
        "        self.net = nn.Sequential(*self.children_list)\n",
        "        self.pretrained = None\n",
        "        \n",
        "    def forward(self,x):\n",
        "        x = self.net(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "O0vT7v73V9cd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Создаём объект-экстрактор с выборанным слоем"
      ],
      "metadata": {
        "id": "vnR8wWKSpRhV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resnet_extractor = FeatureExtractor(output_layer='avgpool', torch_model=rn18)\n"
      ],
      "metadata": {
        "id": "NAiIQvreOvL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Проверяем директорию с картинками"
      ],
      "metadata": {
        "id": "IO1wL1d4Fs2E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ROOT_MEMES_DIR = os.path.join(ROOT_DIR, 'MyDrive', 'hse_nlp_2021', 'memes')\n",
        "\n",
        "os.listdir(ROOT_MEMES_DIR)[:10]"
      ],
      "metadata": {
        "id": "sJXkiao5FtGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "\n",
        "filename = os.path.join(ROOT_MEMES_DIR, '7f3ywc')\n",
        "\n",
        "input_image = Image.open(filename)"
      ],
      "metadata": {
        "id": "cTVdn7QPNG18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(input_image)"
      ],
      "metadata": {
        "id": "UDjz-L5gyur_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import no_grad, cuda\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# transform = transforms.ToTensor()\n",
        "\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "image_tensor = preprocess(input_image)\n",
        "input_batch = image_tensor.unsqueeze(0) # create a mini-batch as expected by the model\n",
        "\n",
        "# move the input and model to GPU for speed if available\n",
        "if cuda.is_available():\n",
        "    input_batch = input_batch.to('cuda')\n",
        "    resnet_extractor.to('cuda')\n",
        "\n",
        "with no_grad():\n",
        "    output = resnet_extractor(input_batch)\n",
        "    numpy_vector = output.reshape(-1).cpu().numpy()  # flatten(output)\n",
        "    print(type(output), output.size())"
      ],
      "metadata": {
        "id": "jwJLliIoV9fm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms as transforms\n",
        "from PIL.JpegImagePlugin import JpegImageFile\n",
        "from torch import no_grad, cuda\n",
        "from PIL import Image, UnidentifiedImageError\n",
        "\n",
        "def img2embedding(input_meme_filename: str) -> np.array:\n",
        "  OUTPUT_SHAPE = 512\n",
        "  numpy_vector = np.zeros(OUTPUT_SHAPE)\n",
        "  preprocess = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "  ])\n",
        "\n",
        "  try:\n",
        "    input_img = Image.open(input_meme_filename) # type: JpegImageFile\n",
        "  except UnidentifiedImageError:\n",
        "    return numpy_vector\n",
        "  try:\n",
        "    image_tensor = preprocess(input_img)\n",
        "  except RuntimeError:\n",
        "    #  logger.info('error with %s meme', meme_filename.split('/')[-1])\n",
        "    return numpy_vector\n",
        "  input_batch = image_tensor.unsqueeze(0) # create a mini-batch as expected by the model\n",
        "\n",
        "  # move the input and model to GPU for speed if available\n",
        "  if cuda.is_available():\n",
        "      input_batch = input_batch.to('cuda')\n",
        "      resnet_extractor.to('cuda')\n",
        "\n",
        "  with no_grad():\n",
        "      output = resnet_extractor(input_batch)\n",
        "      numpy_vector = output.reshape(-1).cpu().numpy()  # flatten(output)\n",
        "  return numpy_vector\n",
        "\n",
        "\n",
        "if os.path.exists(os.path.join(ROOT_MEMES_DIR, 'embed.npy')):\n",
        "  embeds_matrix = np.load(os.path.join(ROOT_MEMES_DIR, 'embed.npy'))\n",
        "  with open(os.path.join(ROOT_MEMES_DIR, 'file_index.pkl'), 'rb') as f:\n",
        "    file_index = pickle.load(f)\n",
        "    logger.info('files loaded from dump')\n",
        "else:\n",
        "  res = []  # тут основная информация о контенте\n",
        "  file_index = {}\n",
        "  TOP = 3092\n",
        "  error_files = []\n",
        "  logger.info('Processing started')\n",
        "  dense_index = 0\n",
        "  for f_name in os.listdir(ROOT_MEMES_DIR)[:TOP]:\n",
        "    meme_filename = os.path.join(ROOT_MEMES_DIR, f_name)\n",
        "    img_embed = img2embedding(meme_filename)\n",
        "    if img_embed.sum() == 0:\n",
        "      error_files.append(meme_filename)\n",
        "    # сохраняяем эмбеддинг (их потом схлопнем в матрицу) и отдельно индекс файла в матрице\n",
        "    res.append(img_embed)\n",
        "    file_index[dense_index] = {'f_name': f_name}\n",
        "    dense_index += 1\n",
        "    \n",
        "  if len(error_files) > 0:\n",
        "    logger.info('num errors %d', len(error_files))\n",
        "    for i in error_files:\n",
        "      os.remove(i)\n",
        "\n",
        "  embeds_matrix = np.vstack(res)\n",
        "\n",
        "logger.info(embeds_matrix.shape)"
      ],
      "metadata": {
        "id": "Q7srLON2V9jL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "np.save(os.path.join(ROOT_MEMES_DIR, 'embed.npy') , embeds_matrix)\n",
        "with open(os.path.join(ROOT_MEMES_DIR, 'file_index.pkl'), 'wb') as f:\n",
        "  pickle.dump(file_index, f)"
      ],
      "metadata": {
        "id": "MOg4T8CSy6mE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Устанавливаем umap \n",
        "\n",
        "пример из [официальной документации](https://umap-learn.readthedocs.io/en/latest/basic_usage.html)"
      ],
      "metadata": {
        "id": "m1oNoJ7r4NrQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install umap-learn==0.5.2"
      ],
      "metadata": {
        "id": "vIB3Gio54bEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import umap\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "reducer = umap.UMAP()\n",
        "\n",
        "scaled_memes_data = StandardScaler().fit_transform(embeds_matrix)\n",
        "low_rank_matrix = reducer.fit_transform(scaled_memes_data)\n",
        "logger.info('low rank matrix shape %s', low_rank_matrix.shape)"
      ],
      "metadata": {
        "id": "xkKLwLXjV9pN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "plt.scatter(\n",
        "    low_rank_matrix[:, 0],\n",
        "    low_rank_matrix[:, 1],\n",
        ")\n",
        "plt.gca().set_aspect('equal', 'datalim')\n",
        "plt.title('UMAP projection of the memes dataset', fontsize=24)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IohdafZC4BGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Получился один \"кластер\"\n",
        "\n",
        "Выполняем кластеризацию в низкоразмерном пространстве с помощью DBScan чтобы выделить метки кластеров"
      ],
      "metadata": {
        "id": "RH_jWch79kwL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "clstr = DBSCAN(eps=0.10, min_samples=4)\n",
        "classes = clstr.fit_predict(low_rank_matrix)\n",
        "logger.info('num classes %s', np.unique(classes).size)\n",
        "\n",
        "plt.scatter(\n",
        "    low_rank_matrix[:, 0],\n",
        "    low_rank_matrix[:, 1],\n",
        "    c=classes,\n",
        "    cmap='rainbow',\n",
        "    alpha=0.7,\n",
        ")\n",
        "plt.gca().set_aspect('equal', 'datalim')\n",
        "plt.title('UMAP projection of the memes dataset', fontsize=24)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Jnd8NpVU82a5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "clusters_filename = os.path.join(ROOT_MEMES_DIR, 'dbscan_clusters.csv')\n",
        "if os.path.exists(clusters_filename):\n",
        "  memes_df = pd.read_csv(clusters_filename)\n",
        "  logger.info('loaded_from %s', clusters_filename)\n",
        "else:\n",
        "  df_rows = []\n",
        "  for meme_index in range(low_rank_matrix.shape[0]):\n",
        "    df_rows.append((file_index[meme_index]['f_name'], classes[meme_index]))\n",
        "  memes_df = pd.DataFrame(df_rows, columns=['f_name', 'dbscan_cluster'])\n",
        "logger.info('%s', memes_df['dbscan_cluster'].value_counts().head(10).to_dict())\n",
        "memes_df.head(5)"
      ],
      "metadata": {
        "id": "hDMlsmsH95Wp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memes_df.to_csv(os.path.join(ROOT_MEMES_DIR, 'dbscan_clusters.csv'), index=False)"
      ],
      "metadata": {
        "id": "KQCktlEc_kDg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Видно, что есть кластер с индексом `0` где большая часть контента и меньшие по можности кластера. ДЛя сравнения визуализируем кластер c индексом `3` и кластер с индексом `4`"
      ],
      "metadata": {
        "id": "PgXRufB2wqcy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "for f_name in os.listdir(ROOT_MEMES_DIR):\n",
        "  if f_name not in ('file_index.pkl', 'embed.npy'):\n",
        "    os.rename(os.path.join(ROOT_MEMES_DIR, f_name), os.path.join(ROOT_MEMES_DIR, f_name+'.jpeg'))"
      ],
      "metadata": {
        "id": "FF2rdTNY-00U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image as NotebookImage\n",
        "from IPython.display import display\n",
        "\n",
        "def visualise_cluster(cluster_id: int, top=10):\n",
        "  for _, row in memes_df.query(f'dbscan_cluster == {cluster_id}').head(top).iterrows():\n",
        "    tmp_file_path = os.path.join(ROOT_MEMES_DIR, row['f_name']+'.jpeg')\n",
        "    pil_img = NotebookImage(filename=tmp_file_path, width=200)\n",
        "    display(pil_img)\n",
        "\n",
        "visualise_cluster(cluster_id=14)"
      ],
      "metadata": {
        "id": "lBYpBbhsnLyt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualise_cluster(cluster_id=16)"
      ],
      "metadata": {
        "id": "vo-c3xLGxuUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualise_cluster(cluster_id=5)"
      ],
      "metadata": {
        "id": "ihMWhvMCCaas"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "4Y_DGvqiEvK_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}